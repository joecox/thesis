\chapter{Results}
We evaluated our approach by comparing the number of tracked methods produced by the analysis of the prior partial instrumentation work to the number of tracked methods produced by the analysis in this paper.

\section{Benchmark set}
While Phosphor \cite{phosphor_oopsla} was evaluated on performance using all 14 benchmarks of the  DaCapo benchmark suite \cite{dacapobach} included in the 9.12-bach release, \cite{manoj_project} was evaluated on performance using only 7 of the 14 benchmarks: avrora, batik, h2, pmd, sunflow, tomcat, and xalan, due to numerous problems with bytecode verification for the remaining 7. However, lists of tracked methods were generated for all 14 of the benchmarks in the \cite{manoj_project}, so we can compare tracked method counts between that project and this paper. The lists of tracked methods produced in \cite{manoj_project} are probably not completely accurate because more code may have had to be instrumented to solve the bytecode verification issues. However, the comparison can still be made because, if anything, the method lists of \cite{manoj_project} would only have been larger. If the tracked method counts from this paper's analysis are favorable compared to those of \cite{manoj_project}, then they would only be more favorable if the method lists from \cite{manoj_project} were larger.

\section{Method counts}

\begin{center}
  \begin{tabular}{|c|c|c|}
    \hline
    Benchmark & \cite{manoj_project} & This paper \\ \hline
    avrora & 46948 & 6724 \\\hline
    batik & 50625 & x \\\hline
    eclipse & 21776 & x \\\hline
    fop & 52183 & x \\\hline
    h2 & 49765 & x \\\hline
    jython & 55125 & x \\\hline
    luindex & 21397 & x \\\hline
    lusearch & 21524 & x \\\hline
    pmd & 47148 & x \\\hline
    sunflow & 45700 & x \\\hline
    tomcat & 45935 & x \\\hline
    tradebeans & 45953 & x \\\hline
    tradesoap & 45786 & x \\\hline
    xalan & 48437 & x \\\hline
  \end{tabular}
  \captionof{table}{Tracked method counts of \cite{manoj_project} and this paper's analysis}
\end{center}

Table 4.1 shows the tracked method counts produced by the previous analysis created in \cite{manoj_project} and the analysis in this paper. The tracked methods are the methods that will become instrumented by Phosphor to track taint propagation through the method. Remaining untracked methods only contain local variables which remain always tainted or untainted for the duration of the method and as such we do not need to track taint. Because instrumentation of methods is the primary contributer to the performance overhead of Phosphor and in \cite{manoj_phosphor}, comparing the tracked method counts of the previous analysis and the analysis in this paper is a good proxy for comparing performance overhead.

\section{Benchmarking Phosphor}
While the above measurements of trackd method counts are good proxies for comparing performance, they do not directly measure real-world performance of the modified Phosphor tool described in section 3.6. Additional evaluation of the analysis approach in this paper can be done by measuring runtime performance of Phosphor on the Dacapo benchmarks given the information on which methods to track and which call sites to instrument. These performance results were not obtained due to time restrictions and other confounding factors.
